{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import timm\n",
    "import wandb\n",
    "import torch.utils.data as data\n",
    "import gdal\n",
    "import rasterio\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "#import sklearn.metrics\n",
    "\n",
    "\n",
    "# New Libraries\n",
    "#import boto3\n",
    "#from s3fs.core import S3FileSystem\n",
    "\n",
    "plt.ion()   # interactive mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/capstone_final_pct_water/notebooks'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/capstone_final_pct_water/notebooks/data/amazon_tiff_data/train/1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ac3a2553349b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~/capstone_final_pct_water/notebooks/data/amazon_tiff_data/train/1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/capstone_final_pct_water/notebooks/data/amazon_tiff_data/train/1'"
     ]
    }
   ],
   "source": [
    "# with open(\"~/capstone_final_pct_water/notebooks/data/amazon_tiff_data/train/1\", 'rb') as f:\n",
    "#         img = Image.open(f)\n",
    "#         img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating VisionDataset, StandardTransform, DataFolder and ImageFolder functions\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "class VisionDataset(data.Dataset):\n",
    "    _repr_indent = 4\n",
    "\n",
    "    def __init__(self, root, transforms=None, transform=None, target_transform=None):\n",
    "        if isinstance(root, torch._six.string_classes):\n",
    "            root = os.path.expanduser(root)\n",
    "        self.root = root\n",
    "\n",
    "        has_transforms = transforms is not None\n",
    "        has_separate_transform = transform is not None or target_transform is not None\n",
    "        if has_transforms and has_separate_transform:\n",
    "            raise ValueError(\"Only transforms or transform/target_transform can \"\n",
    "                             \"be passed as argument\")\n",
    "\n",
    "        # for backwards-compatibility\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if has_separate_transform:\n",
    "            transforms = StandardTransform(transform, target_transform)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self):\n",
    "        head = \"Dataset \" + self.__class__.__name__\n",
    "        body = [\"Number of datapoints: {}\".format(self.__len__())]\n",
    "        if self.root is not None:\n",
    "            body.append(\"Root location: {}\".format(self.root))\n",
    "        body += self.extra_repr().splitlines()\n",
    "        if hasattr(self, \"transforms\") and self.transforms is not None:\n",
    "            body += [repr(self.transforms)]\n",
    "        lines = [head] + [\" \" * self._repr_indent + line for line in body]\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def _format_transform_repr(self, transform, head):\n",
    "        lines = transform.__repr__().splitlines()\n",
    "        return ([\"{}{}\".format(head, lines[0])] +\n",
    "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"\"\n",
    "\n",
    "class StandardTransform(object):\n",
    "    def __init__(self, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __call__(self, input, target):\n",
    "        if self.transform is not None:\n",
    "            input = self.transform(input)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return input, target\n",
    "\n",
    "    def _format_transform_repr(self, transform, head):\n",
    "        lines = transform.__repr__().splitlines()\n",
    "        return ([\"{}{}\".format(head, lines[0])] +\n",
    "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
    "\n",
    "    def __repr__(self):\n",
    "        body = [self.__class__.__name__]\n",
    "        if self.transform is not None:\n",
    "            body += self._format_transform_repr(self.transform,\n",
    "                                                \"Transform: \")\n",
    "        if self.target_transform is not None:\n",
    "            body += self._format_transform_repr(self.target_transform,\n",
    "                                                \"Target transform: \")\n",
    "\n",
    "        return '\\n'.join(body)\n",
    "\n",
    "def has_file_allowed_extension(filename, extensions):\n",
    "    \"\"\"Checks if a file is an allowed extension.\n",
    "    Args:\n",
    "        filename (string): path to a file\n",
    "        extensions (tuple of strings): extensions to consider (lowercase)\n",
    "    Returns:\n",
    "        bool: True if the filename ends with one of given extensions\n",
    "    \"\"\"\n",
    "    return filename.lower().endswith(extensions)\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    \"\"\"Checks if a file is an allowed image extension.\n",
    "    Args:\n",
    "        filename (string): path to a file\n",
    "    Returns:\n",
    "        bool: True if the filename ends with a known image extension\n",
    "    \"\"\"\n",
    "    return has_file_allowed_extension(filename, IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def make_dataset(dir, class_to_idx, extensions=None, is_valid_file=None):\n",
    "    images = []\n",
    "    dir = os.path.expanduser(dir)\n",
    "    if not ((extensions is None) ^ (is_valid_file is None)):\n",
    "        raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
    "    if extensions is not None:\n",
    "        def is_valid_file(x):\n",
    "            return has_file_allowed_extension(x, extensions)\n",
    "    for target in sorted(class_to_idx.keys()):\n",
    "        d = os.path.join(dir, target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "        for root, _, fnames in sorted(os.walk(d, followlinks=True)):\n",
    "            for fname in sorted(fnames):\n",
    "                path = os.path.join(root, fname)\n",
    "                if is_valid_file(path):\n",
    "                    item = (path, class_to_idx[target])\n",
    "                    images.append(item)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "class DatasetFolder(VisionDataset):\n",
    "    \"\"\"A generic data loader where the samples are arranged in this way: ::\n",
    "        root/class_x/xxx.ext\n",
    "        root/class_x/xxy.ext\n",
    "        root/class_x/xxz.ext\n",
    "        root/class_y/123.ext\n",
    "        root/class_y/nsdf3.ext\n",
    "        root/class_y/asd932_.ext\n",
    "    Args:\n",
    "        root (string): Root directory path.\n",
    "        loader (callable): A function to load a sample given its path.\n",
    "        extensions (tuple[string]): A list of allowed extensions.\n",
    "            both extensions and is_valid_file should not be passed.\n",
    "        transform (callable, optional): A function/transform that takes in\n",
    "            a sample and returns a transformed version.\n",
    "            E.g, ``transforms.RandomCrop`` for images.\n",
    "        target_transform (callable, optional): A function/transform that takes\n",
    "            in the target and transforms it.\n",
    "        is_valid_file (callable, optional): A function that takes path of a file\n",
    "            and check if the file is a valid file (used to check of corrupt files)\n",
    "            both extensions and is_valid_file should not be passed.\n",
    "     Attributes:\n",
    "        classes (list): List of the class names.\n",
    "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
    "        samples (list): List of (sample path, class_index) tuples\n",
    "        targets (list): The class_index value for each image in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, loader, extensions=None, transform=None,\n",
    "                 target_transform=None, is_valid_file=None):\n",
    "        super(DatasetFolder, self).__init__(root, transform=transform,\n",
    "                                            target_transform=target_transform)\n",
    "        classes, class_to_idx = self._find_classes(self.root)\n",
    "        samples = make_dataset(self.root, class_to_idx, extensions, is_valid_file)\n",
    "        if len(samples) == 0:\n",
    "            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n",
    "                                \"Supported extensions are: \" + \",\".join(extensions)))\n",
    "\n",
    "        self.loader = loader\n",
    "        self.extensions = extensions\n",
    "\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.samples = samples\n",
    "        self.targets = [s[1] for s in samples]\n",
    "\n",
    "    def _find_classes(self, dir):\n",
    "        \"\"\"\n",
    "        Finds the class folders in a dataset.\n",
    "        Args:\n",
    "            dir (string): Root directory path.\n",
    "        Returns:\n",
    "            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
    "        Ensures:\n",
    "            No class is a subdirectory of another.\n",
    "        \"\"\"\n",
    "        if sys.version_info >= (3, 5):\n",
    "            # Faster and available in Python 3.5 and above\n",
    "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        else:\n",
    "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp', '.TIF')\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGBA')\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "class ImageFolder(DatasetFolder):\n",
    "    \"\"\"A generic data loader where the images are arranged in this way: ::\n",
    "        root/dog/xxx.png\n",
    "        root/dog/xxy.png\n",
    "        root/dog/xxz.png\n",
    "        root/cat/123.png\n",
    "        root/cat/nsdf3.png\n",
    "        root/cat/asd932_.png\n",
    "    Args:\n",
    "        root (string): Root directory path.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an image given its path.\n",
    "        is_valid_file (callable, optional): A function that takes path of an Image file\n",
    "            and check if the file is a valid file (used to check of corrupt files)\n",
    "     Attributes:\n",
    "        classes (list): List of the class names.\n",
    "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
    "        imgs (list): List of (image path, class_index) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None, target_transform=None,loader=default_loader, is_valid_file=None):\n",
    "        super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
    "                                          transform=transform,\n",
    "                                          target_transform=target_transform,\n",
    "                                          is_valid_file=is_valid_file)\n",
    "        self.imgs = self.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Load Data\n",
    "# ---------\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "# WE WILL WANT TO UPDATE NORMALIZATION\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.7475, 0.6532, 0.4753, 0.0961], [0.1673, 0.1827, 0.2137, 0.0286])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.7475, 0.6532, 0.4753, 0.0961], [0.1673, 0.1827, 0.2137, 0.0286])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "data_dir = '~/data/australia'\n",
    "\n",
    "image_datasets = {x: ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "dataloaders = {x: data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = 0.0\n",
    "# for images, _ in dataloaders['val']:\n",
    "#     batch_samples = images.size(0) \n",
    "#     images = images.view(batch_samples, images.size(1), -1)\n",
    "#     mean += images.mean(2).sum(0)\n",
    "# mean = mean / len(dataloaders['val'].dataset)\n",
    "\n",
    "# var = 0.0\n",
    "# for images, _ in dataloaders['val']:\n",
    "#     batch_samples = images.size(0)\n",
    "#     images = images.view(batch_samples, images.size(1), -1)\n",
    "#     var += ((images - mean.unsqueeze(1))**2).sum([0,2])\n",
    "# std = torch.sqrt(var / (len(dataloaders['val'].dataset)*224*224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7492, 0.6541, 0.4755, 0.0961])\n",
      "tensor([0.1671, 0.1819, 0.2122, 0.0282])\n"
     ]
    }
   ],
   "source": [
    "# print(mean)\n",
    "# print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255, 256, 4)\n"
     ]
    }
   ],
   "source": [
    "serengeti = '/home/ubuntu/data/serengeti/train/0/20200208_074839_0f15_3B_AnalyticMS_SR_clip_95.tif'\n",
    "\n",
    "with rasterio.open(serengeti) as f:\n",
    "    dataset = f.read().astype(np.uint8)\n",
    "    dataset = reshape_as_image(dataset)\n",
    "    print(dataset.shape)\n",
    "#     dataset = np.asarray(dataset)\n",
    "#     print(dataset)\n",
    "#     img = Image.fromarray(dataset, \"RGBA\")\n",
    "#     img.convert(\"RGBA\")\n",
    "#     print(img)\n",
    "#         return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Caught OSError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-12-7485f66fd6bd>\", line 200, in __getitem__\n    sample = self.loader(path)\n  File \"<ipython-input-12-7485f66fd6bd>\", line 236, in default_loader\n    return pil_loader(path)\n  File \"<ipython-input-12-7485f66fd6bd>\", line 218, in pil_loader\n    img = Image.open(f)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/PIL/Image.py\", line 2687, in open\n    % (filename if filename else fp))\nOSError: cannot identify image file <_io.BufferedReader name='/home/ubuntu/data/australia/train/0/874754_5530810_2017_10_28_0f18_BGRN_Analytic_metadata_TOA_13294.TIF'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9ee4eddded6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Get a batch of training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Make a grid from batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: Caught OSError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-12-7485f66fd6bd>\", line 200, in __getitem__\n    sample = self.loader(path)\n  File \"<ipython-input-12-7485f66fd6bd>\", line 236, in default_loader\n    return pil_loader(path)\n  File \"<ipython-input-12-7485f66fd6bd>\", line 218, in pil_loader\n    img = Image.open(f)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/PIL/Image.py\", line 2687, in open\n    % (filename if filename else fp))\nOSError: cannot identify image file <_io.BufferedReader name='/home/ubuntu/data/australia/train/0/874754_5530810_2017_10_28_0f18_BGRN_Analytic_metadata_TOA_13294.TIF'>\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Visualize a few images\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Let's visualize a few training images so as to understand the data\n",
    "# augmentations.\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.7475, 0.6532, 0.4753, 0.0961])\n",
    "    std = np.array([0.1673, 0.1827, 0.2137, 0.0286])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    wandb.init(project=\"serengeti-water\")\n",
    "    wandb.watch(model)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            true_pos = 0\n",
    "            true_neg = 0\n",
    "            false_pos = 0\n",
    "            false_neg = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics -- adding \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                for i,j in enumerate(preds):\n",
    "                    true_pos += torch.sum(j == 1 and labels.data[i] == 1)\n",
    "                    true_neg += torch.sum(j == 0 and labels.data[i] == 0)\n",
    "                    false_pos += torch.sum(j == 1 and labels.data[i] == 0)\n",
    "                    false_neg += torch.sum(j == 0 and labels.data[i] == 1)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_prec = true_pos.double()/(true_pos.double() + false_pos.double())\n",
    "            epoch_recall = true_pos.double()/(true_pos.double() + false_neg.double())\n",
    "            epoch_f1 = 2 * (epoch_prec * epoch_recall) / (epoch_prec + epoch_recall)            \n",
    "            \n",
    "            \n",
    "#             epoch_prec = sklearn.metrics.precision_score(labels.data, preds)\n",
    "#             epoch_recall = sklearn.metrics.recall_score(labels.data, preds)\n",
    "#             epoch_f1 = sklearn.metrics.f1_score(labels.data, preds)\n",
    "#             print('{} Loss: {:.4f} | Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('{} Loss: {:.4f} | Acc: {:.4f} | Prec: {:.4f} | Recall: {:.4f} | F1: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc, epoch_prec, epoch_recall, epoch_f1))\n",
    "            wandb.log({'{} Loss'.format(phase): epoch_loss, '{} Accuracy'.format(phase): epoch_acc, '{} Precision'.format(phase): epoch_prec, '{} Recall'.format(phase): epoch_recall, '{} F1-Score'.format(phase): epoch_f1})\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Finetuning the convnet\n",
    "# ----------------------\n",
    "#\n",
    "# Load a pretrained model and reset final fully connected layer.\n",
    "#\n",
    "\n",
    "model_ft = timm.create_model('mixnet_l', pretrained=True)\n",
    "model_ft.conv_stem = nn.Conv2d(4,32, kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False)\n",
    "\n",
    "num_ftrs = model_ft.classifier.in_features\n",
    "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "#model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/pcoggins/serengeti-water\" target=\"_blank\">https://app.wandb.ai/pcoggins/serengeti-water</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/pcoggins/serengeti-water/runs/cuti3teu\" target=\"_blank\">https://app.wandb.ai/pcoggins/serengeti-water/runs/cuti3teu</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.2835 | Acc: 0.8925 | Prec: 0.7789 | Recall: 0.5796 | F1: 0.6647\n",
      "val Loss: 0.2590 | Acc: 0.9077 | Prec: 0.7856 | Recall: 0.6532 | F1: 0.7133\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.2394 | Acc: 0.9101 | Prec: 0.8126 | Recall: 0.6644 | F1: 0.7311\n",
      "val Loss: 0.2474 | Acc: 0.9087 | Prec: 0.7880 | Recall: 0.6574 | F1: 0.7168\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.2286 | Acc: 0.9140 | Prec: 0.8206 | Recall: 0.6810 | F1: 0.7443\n",
      "val Loss: 0.2410 | Acc: 0.9087 | Prec: 0.7541 | Recall: 0.7131 | F1: 0.7330\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.2164 | Acc: 0.9180 | Prec: 0.8268 | Recall: 0.7010 | F1: 0.7587\n",
      "val Loss: 0.2395 | Acc: 0.9094 | Prec: 0.7559 | Recall: 0.7159 | F1: 0.7353\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.2100 | Acc: 0.9202 | Prec: 0.8346 | Recall: 0.7056 | F1: 0.7647\n",
      "val Loss: 0.2338 | Acc: 0.9099 | Prec: 0.7493 | Recall: 0.7326 | F1: 0.7408\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.2036 | Acc: 0.9214 | Prec: 0.8360 | Recall: 0.7123 | F1: 0.7692\n",
      "val Loss: 0.2436 | Acc: 0.9070 | Prec: 0.7770 | Recall: 0.6602 | F1: 0.7139\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1984 | Acc: 0.9243 | Prec: 0.8415 | Recall: 0.7250 | F1: 0.7789\n",
      "val Loss: 0.2452 | Acc: 0.9035 | Prec: 0.7238 | Recall: 0.7298 | F1: 0.7268\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1817 | Acc: 0.9306 | Prec: 0.8595 | Recall: 0.7443 | F1: 0.7978\n",
      "val Loss: 0.2351 | Acc: 0.9067 | Prec: 0.7453 | Recall: 0.7131 | F1: 0.7288\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1795 | Acc: 0.9315 | Prec: 0.8625 | Recall: 0.7463 | F1: 0.8002\n",
      "val Loss: 0.2367 | Acc: 0.9087 | Prec: 0.7571 | Recall: 0.7075 | F1: 0.7315\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1769 | Acc: 0.9317 | Prec: 0.8588 | Recall: 0.7525 | F1: 0.8022\n",
      "val Loss: 0.2314 | Acc: 0.9106 | Prec: 0.7797 | Recall: 0.6852 | F1: 0.7294\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1744 | Acc: 0.9330 | Prec: 0.8623 | Recall: 0.7563 | F1: 0.8058\n",
      "val Loss: 0.2401 | Acc: 0.9074 | Prec: 0.7422 | Recall: 0.7256 | F1: 0.7338\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1734 | Acc: 0.9335 | Prec: 0.8650 | Recall: 0.7566 | F1: 0.8072\n",
      "val Loss: 0.2413 | Acc: 0.9048 | Prec: 0.7301 | Recall: 0.7270 | F1: 0.7285\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1687 | Acc: 0.9352 | Prec: 0.8634 | Recall: 0.7691 | F1: 0.8136\n",
      "val Loss: 0.2475 | Acc: 0.8972 | Prec: 0.6930 | Recall: 0.7451 | F1: 0.7181\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1673 | Acc: 0.9356 | Prec: 0.8686 | Recall: 0.7654 | F1: 0.8137\n",
      "val Loss: 0.2469 | Acc: 0.9045 | Prec: 0.7398 | Recall: 0.7047 | F1: 0.7218\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1633 | Acc: 0.9367 | Prec: 0.8669 | Recall: 0.7748 | F1: 0.8183\n",
      "val Loss: 0.2492 | Acc: 0.8996 | Prec: 0.7121 | Recall: 0.7201 | F1: 0.7161\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1648 | Acc: 0.9366 | Prec: 0.8693 | Recall: 0.7711 | F1: 0.8172\n",
      "val Loss: 0.2360 | Acc: 0.9055 | Prec: 0.7406 | Recall: 0.7117 | F1: 0.7259\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1649 | Acc: 0.9363 | Prec: 0.8684 | Recall: 0.7703 | F1: 0.8164\n",
      "val Loss: 0.2439 | Acc: 0.8991 | Prec: 0.7102 | Recall: 0.7201 | F1: 0.7151\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1657 | Acc: 0.9362 | Prec: 0.8654 | Recall: 0.7732 | F1: 0.8167\n",
      "val Loss: 0.2479 | Acc: 0.9028 | Prec: 0.7238 | Recall: 0.7228 | F1: 0.7233\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1632 | Acc: 0.9367 | Prec: 0.8688 | Recall: 0.7727 | F1: 0.8179\n",
      "val Loss: 0.2407 | Acc: 0.9048 | Prec: 0.7288 | Recall: 0.7298 | F1: 0.7293\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1653 | Acc: 0.9366 | Prec: 0.8687 | Recall: 0.7721 | F1: 0.8176\n",
      "val Loss: 0.2491 | Acc: 0.9030 | Prec: 0.7287 | Recall: 0.7145 | F1: 0.7215\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1632 | Acc: 0.9383 | Prec: 0.8732 | Recall: 0.7772 | F1: 0.8224\n",
      "val Loss: 0.2474 | Acc: 0.9035 | Prec: 0.7256 | Recall: 0.7256 | F1: 0.7256\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1645 | Acc: 0.9363 | Prec: 0.8662 | Recall: 0.7732 | F1: 0.8171\n",
      "val Loss: 0.2439 | Acc: 0.9033 | Prec: 0.7365 | Recall: 0.7006 | F1: 0.7181\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1633 | Acc: 0.9372 | Prec: 0.8691 | Recall: 0.7750 | F1: 0.8193\n",
      "val Loss: 0.2532 | Acc: 0.8981 | Prec: 0.6992 | Recall: 0.7382 | F1: 0.7182\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1618 | Acc: 0.9366 | Prec: 0.8653 | Recall: 0.7763 | F1: 0.8184\n",
      "val Loss: 0.2464 | Acc: 0.9043 | Prec: 0.7293 | Recall: 0.7242 | F1: 0.7268\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1633 | Acc: 0.9376 | Prec: 0.8688 | Recall: 0.7781 | F1: 0.8210\n",
      "val Loss: 0.2387 | Acc: 0.9065 | Prec: 0.7435 | Recall: 0.7145 | F1: 0.7287\n",
      "Training complete in 554m 35s\n",
      "Best val Acc: 0.910627\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Train and evaluate\n",
    "# ^^^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# It should take around 15-25 min on CPU. On GPU though, it takes less than a\n",
    "# minute.\n",
    "#\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "#visualize_model(model_ft)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Visualizing the model predictions\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# Generic function to display predictions for a few images\n",
    "#\n",
    "\n",
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# ConvNet as fixed feature extractor\n",
    "# ----------------------------------\n",
    "#\n",
    "# Here, we need to freeze all the network except the final layer. We need\n",
    "# to set ``requires_grad == False`` to freeze the parameters so that the\n",
    "# gradients are not computed in ``backward()``.\n",
    "#\n",
    "# You can read more about this in the documentation\n",
    "# `here <https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward>`__.\n",
    "#\n",
    "\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Train and evaluate\n",
    "# ^^^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# On CPU this will take about half the time compared to previous scenario.\n",
    "# This is expected as gradients don't need to be computed for most of the\n",
    "# network. However, forward does need to be computed.\n",
    "#\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)\n",
    "\n",
    "######################################################################\n",
    "#\n",
    "\n",
    "visualize_model(model_conv)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "######################################################################\n",
    "# Further Learning\n",
    "# -----------------\n",
    "#\n",
    "# If you would like to learn more about the applications of transfer learning,\n",
    "# checkout our `Quantized Transfer Learning for Computer Vision Tutorial <https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html>`_.\n",
    "#\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
