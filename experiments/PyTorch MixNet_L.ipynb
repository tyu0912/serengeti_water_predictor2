{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling in PyTorch implementation of MixNet_L\n",
    "# Resource: https://github.com/romulus0914/MixNet-Pytorch/blob/master/mixnet.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "NON_LINEARITY = {\n",
    "    'ReLU': nn.ReLU(inplace=True),\n",
    "    'Swish': Swish(),\n",
    "}\n",
    "\n",
    "def _RoundChannels(c, divisor=8, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_c = max(min_value, int(c + divisor / 2) // divisor * divisor)\n",
    "    if new_c < 0.9 * c:\n",
    "        new_c += divisor\n",
    "    return new_c\n",
    "\n",
    "def _SplitChannels(channels, num_groups):\n",
    "    split_channels = [channels//num_groups for _ in range(num_groups)]\n",
    "    split_channels[0] += channels - sum(split_channels)\n",
    "    return split_channels\n",
    "\n",
    "def Conv3x3Bn(in_channels, out_channels, stride, non_linear='ReLU'):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        NON_LINEARITY[non_linear]\n",
    "    )\n",
    "\n",
    "def Conv1x1Bn(in_channels, out_channels, non_linear='ReLU'):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        NON_LINEARITY[non_linear]\n",
    "    )\n",
    "\n",
    "class SqueezeAndExcite(nn.Module):\n",
    "    def __init__(self, channels, squeeze_channels, se_ratio):\n",
    "        super(SqueezeAndExcite, self).__init__()\n",
    "\n",
    "        squeeze_channels = squeeze_channels * se_ratio\n",
    "        if not squeeze_channels.is_integer():\n",
    "            raise ValueError('channels must be divisible by 1/ratio')\n",
    "\n",
    "        squeeze_channels = int(squeeze_channels)\n",
    "        self.se_reduce = nn.Conv2d(channels, squeeze_channels, 1, 1, 0, bias=True)\n",
    "        self.non_linear1 = NON_LINEARITY['Swish']\n",
    "        self.se_expand = nn.Conv2d(squeeze_channels, channels, 1, 1, 0, bias=True)\n",
    "        self.non_linear2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.mean(x, (2, 3), keepdim=True)\n",
    "        y = self.non_linear1(self.se_reduce(y))\n",
    "        y = self.non_linear2(self.se_expand(y))\n",
    "        y = x * y\n",
    "\n",
    "        return y\n",
    "\n",
    "class GroupedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(GroupedConv2d, self).__init__()\n",
    "\n",
    "        self.num_groups = len(kernel_size)\n",
    "        self.split_in_channels = _SplitChannels(in_channels, self.num_groups)\n",
    "        self.split_out_channels = _SplitChannels(out_channels, self.num_groups)\n",
    "\n",
    "        self.grouped_conv = nn.ModuleList()\n",
    "        for i in range(self.num_groups):\n",
    "            self.grouped_conv.append(nn.Conv2d(\n",
    "                self.split_in_channels[i],\n",
    "                self.split_out_channels[i],\n",
    "                kernel_size[i],\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                bias=False\n",
    "            ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_groups == 1:\n",
    "            return self.grouped_conv[0](x)\n",
    "\n",
    "        x_split = torch.split(x, self.split_in_channels, dim=1)\n",
    "        x = [conv(t) for conv, t in zip(self.grouped_conv, x_split)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MDConv(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, stride):\n",
    "        super(MDConv, self).__init__()\n",
    "\n",
    "        self.num_groups = len(kernel_size)\n",
    "        self.split_channels = _SplitChannels(channels, self.num_groups)\n",
    "\n",
    "        self.mixed_depthwise_conv = nn.ModuleList()\n",
    "        for i in range(self.num_groups):\n",
    "            self.mixed_depthwise_conv.append(nn.Conv2d(\n",
    "                self.split_channels[i],\n",
    "                self.split_channels[i],\n",
    "                kernel_size[i],\n",
    "                stride=stride,\n",
    "                padding=kernel_size[i]//2,\n",
    "                groups=self.split_channels[i],\n",
    "                bias=False\n",
    "            ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_groups == 1:\n",
    "            return self.mixed_depthwise_conv[0](x)\n",
    "\n",
    "        x_split = torch.split(x, self.split_channels, dim=1)\n",
    "        x = [conv(t) for conv, t in zip(self.mixed_depthwise_conv, x_split)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MixNetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=[3],\n",
    "        expand_ksize=[1],\n",
    "        project_ksize=[1],\n",
    "        stride=1,\n",
    "        expand_ratio=1,\n",
    "        non_linear='ReLU',\n",
    "        se_ratio=0.0\n",
    "    ):\n",
    "\n",
    "        super(MixNetBlock, self).__init__()\n",
    "\n",
    "        expand = (expand_ratio != 1)\n",
    "        expand_channels = in_channels * expand_ratio\n",
    "        se = (se_ratio != 0.0)\n",
    "        self.residual_connection = (stride == 1 and in_channels == out_channels)\n",
    "\n",
    "        conv = []\n",
    "\n",
    "        if expand:\n",
    "            # expansion phase\n",
    "            pw_expansion = nn.Sequential(\n",
    "                GroupedConv2d(in_channels, expand_channels, expand_ksize),\n",
    "                nn.BatchNorm2d(expand_channels),\n",
    "                NON_LINEARITY[non_linear]\n",
    "            )\n",
    "            conv.append(pw_expansion)\n",
    "\n",
    "        # depthwise convolution phase\n",
    "        dw = nn.Sequential(\n",
    "            MDConv(expand_channels, kernel_size, stride),\n",
    "            nn.BatchNorm2d(expand_channels),\n",
    "            NON_LINEARITY[non_linear]\n",
    "        )\n",
    "        conv.append(dw)\n",
    "\n",
    "        if se:\n",
    "            # squeeze and excite\n",
    "            squeeze_excite = SqueezeAndExcite(expand_channels, in_channels, se_ratio)\n",
    "            conv.append(squeeze_excite)\n",
    "\n",
    "        # projection phase\n",
    "        pw_projection = nn.Sequential(\n",
    "            GroupedConv2d(expand_channels, out_channels, project_ksize),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        conv.append(pw_projection)\n",
    "\n",
    "        self.conv = nn.Sequential(*conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual_connection:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "class MixNet(nn.Module):\n",
    "    # [in_channels, out_channels, kernel_size, expand_ksize, project_ksize, stride, expand_ratio, non_linear, se_ratio]\n",
    "    mixnet_s = [(16,  16,  [3],              [1],    [1],    1, 1, 'ReLU',  0.0),\n",
    "                (16,  24,  [3],              [1, 1], [1, 1], 2, 6, 'ReLU',  0.0),\n",
    "                (24,  24,  [3],              [1, 1], [1, 1], 1, 3, 'ReLU',  0.0),\n",
    "                (24,  40,  [3, 5, 7],        [1],    [1],    2, 6, 'Swish', 0.5),\n",
    "                (40,  40,  [3, 5],           [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (40,  40,  [3, 5],           [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (40,  40,  [3, 5],           [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (40,  80,  [3, 5, 7],        [1],    [1, 1], 2, 6, 'Swish', 0.25),\n",
    "                (80,  80,  [3, 5],           [1],    [1, 1], 1, 6, 'Swish', 0.25),\n",
    "                (80,  80,  [3, 5],           [1],    [1, 1], 1, 6, 'Swish', 0.25),\n",
    "                (80,  120, [3, 5, 7],        [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (120, 120, [3, 5, 7, 9],     [1, 1], [1, 1], 1, 3, 'Swish', 0.5),\n",
    "                (120, 120, [3, 5, 7, 9],     [1, 1], [1, 1], 1, 3, 'Swish', 0.5),\n",
    "                (120, 200, [3, 5, 7, 9, 11], [1],    [1],    2, 6, 'Swish', 0.5),\n",
    "                (200, 200, [3, 5, 7, 9],     [1],    [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (200, 200, [3, 5, 7, 9],     [1],    [1, 1], 1, 6, 'Swish', 0.5)]\n",
    "    \n",
    "    mixnet_m = [(24,  24,  [3],          [1],    [1],    1, 1, 'ReLU',  0.0),\n",
    "                (24,  32,  [3, 5, 7],    [1, 1], [1, 1], 2, 6, 'ReLU',  0.0),\n",
    "                (32,  32,  [3],          [1, 1], [1, 1], 1, 3, 'ReLU',  0.0),\n",
    "                (32,  40,  [3, 5, 7, 9], [1],    [1],    2, 6, 'Swish', 0.5),\n",
    "                (40,  40,  [3, 5],       [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (40,  40,  [3, 5],       [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (40,  40,  [3, 5],       [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (40,  80,  [3, 5, 7],    [1],    [1],    2, 6, 'Swish', 0.25),\n",
    "                (80,  80,  [3, 5, 7, 9], [1, 1], [1, 1], 1, 6, 'Swish', 0.25),\n",
    "                (80,  80,  [3, 5, 7, 9], [1, 1], [1, 1], 1, 6, 'Swish', 0.25),\n",
    "                (80,  80,  [3, 5, 7, 9], [1, 1], [1, 1], 1, 6, 'Swish', 0.25),\n",
    "                (80,  120, [3],          [1],    [1],    1, 6, 'Swish', 0.5),\n",
    "                (120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, 'Swish', 0.5),\n",
    "                (120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, 'Swish', 0.5),\n",
    "                (120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, 'Swish', 0.5),\n",
    "                (120, 200, [3, 5, 7, 9], [1],    [1], 2, 6, 'Swish', 0.5),\n",
    "                (200, 200, [3, 5, 7, 9], [1],    [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (200, 200, [3, 5, 7, 9], [1],    [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (200, 200, [3, 5, 7, 9], [1],    [1, 1], 1, 6, 'Swish', 0.5)]\n",
    "\n",
    "    def __init__(self, net_type='mixnet_s', input_size=224, num_classes=1000, stem_channels=16, feature_size=1536, depth_multiplier=1.0):\n",
    "        super(MixNet, self).__init__()\n",
    "\n",
    "        if net_type == 'mixnet_s':\n",
    "            config = self.mixnet_s\n",
    "            stem_channels = 16\n",
    "            dropout_rate = 0.2\n",
    "        elif net_type == 'mixnet_m':\n",
    "            config = self.mixnet_m\n",
    "            stem_channels = 24\n",
    "            dropout_rate = 0.25\n",
    "        elif net_type == 'mixnet_l':\n",
    "            config = self.mixnet_m\n",
    "            stem_channels = 24\n",
    "            depth_multiplier *= 1.3\n",
    "            dropout_rate = 0.25\n",
    "        else:\n",
    "            raise TypeError('Unsupported MixNet type')\n",
    "\n",
    "        assert input_size % 32 == 0\n",
    "\n",
    "        # depth multiplier\n",
    "        if depth_multiplier != 1.0:\n",
    "            stem_channels = _RoundChannels(stem_channels*depth_multiplier)\n",
    "\n",
    "            for i, conf in enumerate(config):\n",
    "                conf_ls = list(conf)\n",
    "                conf_ls[0] = _RoundChannels(conf_ls[0]*depth_multiplier)\n",
    "                conf_ls[1] = _RoundChannels(conf_ls[1]*depth_multiplier)\n",
    "                config[i] = tuple(conf_ls)\n",
    "\n",
    "        # stem convolution\n",
    "        self.stem_conv = Conv3x3Bn(3, stem_channels, 2)\n",
    "\n",
    "        # building MixNet blocks\n",
    "        layers = []\n",
    "        for in_channels, out_channels, kernel_size, expand_ksize, project_ksize, stride, expand_ratio, non_linear, se_ratio in config:\n",
    "            layers.append(MixNetBlock(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                expand_ksize=expand_ksize,\n",
    "                project_ksize=project_ksize,\n",
    "                stride=stride,\n",
    "                expand_ratio=expand_ratio,\n",
    "                non_linear=non_linear,\n",
    "                se_ratio=se_ratio\n",
    "            ))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # last several layers\n",
    "        self.head_conv = Conv1x1Bn(config[-1][1], feature_size)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(input_size//32, stride=1)\n",
    "        self.classifier = nn.Linear(feature_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem_conv(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.head_conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = MixNet()\n",
    "    x_image = Variable(torch.randn(1, 3, 224, 224))\n",
    "    y = net(x_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-147c9a77bf74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdumpobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloadobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradualWarmupScheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCyclicLR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# Importing Darragh Libraries\n",
    "# Copying print out of environment and base parameter settings\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import optparse\n",
    "import os, sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import cv2\n",
    "import gc\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "'''from albumentations import (Cutout, Compose, Normalize, RandomRotate90, HorizontalFlip,\n",
    "                           VerticalFlip, ShiftScaleRotate, Transpose, OneOf, IAAAdditiveGaussianNoise,\n",
    "                           GaussNoise, RandomGamma, RandomContrast, RandomBrightness, HueSaturationValue,\n",
    "                           RandomCrop, Lambda, NoOp, CenterCrop, Resize\n",
    "                           )'''\n",
    "\n",
    "from tqdm import tqdm\n",
    "#import apex\n",
    "#from apex.parallel import DistributedDataParallel as DDP\n",
    "#from apex.fp16_utils import *\n",
    "#from apex import amp, optimizers\n",
    "#from apex.multi_tensor_apply import multi_tensor_applier\n",
    "#from apex.optimizers.fused_adam import FusedAdam\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Print info about environments\n",
    "# Need to Update rootpath, imgpath, workpath, weightsname\n",
    "parser = optparse.OptionParser()\n",
    "parser.add_option('-s', '--seed', action=\"store\", dest=\"seed\", help=\"model seed\", default=\"1234\")\n",
    "parser.add_option('-o', '--fold', action=\"store\", dest=\"fold\", help=\"Fold for split\", default=\"0\")\n",
    "parser.add_option('-p', '--nbags', action=\"store\", dest=\"nbags\", help=\"Number of bags for averaging\", default=\"0\")\n",
    "parser.add_option('-e', '--epochs', action=\"store\", dest=\"epochs\", help=\"epochs\", default=\"5\")\n",
    "parser.add_option('-b', '--batchsize', action=\"store\", dest=\"batchsize\", help=\"batch size\", default=\"16\")\n",
    "parser.add_option('-r', '--rootpath', action=\"store\", dest=\"rootpath\", help=\"root directory\", default=\"/share/dhanley2/recursion/\")\n",
    "parser.add_option('-i', '--imgpath', action=\"store\", dest=\"imgpath\", help=\"root directory\", default=\"data/mount/512X512X6/\")\n",
    "parser.add_option('-w', '--workpath', action=\"store\", dest=\"workpath\", help=\"Working path\", default=\"densenetv1/weights\")\n",
    "parser.add_option('-f', '--weightsname', action=\"store\", dest=\"weightsname\", help=\"Weights file name\", default=\"pytorch_model.bin\")\n",
    "parser.add_option('-c', '--customwt', action=\"store\", dest=\"customwt\", help=\"Weight of annotator count in loss\", default=\"1.0\")\n",
    "parser.add_option('-l', '--lr', action=\"store\", dest=\"lr\", help=\"learning rate\", default=\"0.00005\")\n",
    "parser.add_option('-t', '--lrmult', action=\"store\", dest=\"lrmult\", help=\"learning rate multiplier\", default=\"4\")\n",
    "parser.add_option('-u', '--cutmix_prob', action=\"store\", dest=\"cutmix_prob\", help=\"Cutmix probability\", default=\"0\")\n",
    "parser.add_option('-a', '--beta', action=\"store\", dest=\"beta\", help=\"Cutmix beta\", default=\"0\")\n",
    "parser.add_option('-n', '--probsname', action=\"store\", dest=\"probsname\", help=\"probs file name\", default=\"probs_256\")\n",
    "parser.add_option('-g', '--logmsg', action=\"store\", dest=\"logmsg\", help=\"root directory\", default=\"Recursion-pytorch\")\n",
    "parser.add_option('-j', '--precision', action=\"store\", dest=\"precision\", help=\"root directory\", default=\"half\")\n",
    "\n",
    "options, args = parser.parse_args()\n",
    "package_dir = options.rootpath\n",
    "sys.path.append(package_dir)\n",
    "from logging import getLogger\n",
    "from utils import dumpobj, loadobj, GradualWarmupScheduler, CyclicLR\n",
    "\n",
    "\n",
    "# Print info about environments\n",
    "logger = getLogger(options.logmsg, 'INFO') # noqa\n",
    "logger.info('Cuda set up : time {}'.format(datetime.datetime.now().time()))\n",
    "\n",
    "device=torch.device('cuda')\n",
    "logger.info('Device : {}'.format(torch.cuda.get_device_name(0)))\n",
    "logger.info('Cuda available : {}'.format(torch.cuda.is_available()))\n",
    "n_gpu = torch.cuda.device_count()\n",
    "logger.info('Cuda n_gpus : {}'.format(n_gpu ))\n",
    "\n",
    "\n",
    "logger.info('Load params : time {}'.format(datetime.datetime.now().time()))\n",
    "for (k,v) in options.__dict__.items():\n",
    "    logger.info('{}{}'.format(k.ljust(20), v))\n",
    "\n",
    "cutmix_prob = float(options.cutmix_prob)\n",
    "beta = float(options.beta)\n",
    "SEED = int(options.seed)\n",
    "EPOCHS = int(options.epochs)\n",
    "lr=float(options.lr)\n",
    "lrmult=int(options.lrmult)\n",
    "batch_size = int(options.batchsize)\n",
    "ROOT = options.rootpath\n",
    "path_data = os.path.join(ROOT, 'data')\n",
    "path_img = os.path.join(ROOT, options.imgpath)\n",
    "WORK_DIR = os.path.join(ROOT, options.workpath)\n",
    "WEIGHTS_NAME = options.weightsname\n",
    "PROBS_NAME = options.probsname\n",
    "PRECISION = options.precision\n",
    "fold = int(options.fold)\n",
    "nbags= int(options.nbags)\n",
    "#classes = 1109\n",
    "device = 'cuda'\n",
    "print('Data path : {}'.format(path_data))\n",
    "print('Image path : {}'.format(path_img))\n",
    "\n",
    "os.environ[\"TORCH_HOME\"] = os.path.join( path_data, 'mount')\n",
    "logger.info(os.system('$TORCH_HOME'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
